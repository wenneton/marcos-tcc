%!TEX root = ../main.tex

Nesta seção serão apresentados os resultados parciais obtidos no teste de modelos baseados em algumas das arquiteturas selecionadas para o escopo deste trabalho. As Seções \ref{sec:lenet} e \ref{sec:alexnet} demonstram os resultados obtidos pelos melhores modelos encontrados com as arquiteturas LeNet e AlexNet, respectivamente. O treinamento destas CNNs foi realizado utilizando os recursos computacionais de um servidor, disponível no LSI, dedicado especialmente para tarefas de DL, o qual possui um processador Intel Core i7 com 16 GB de RAM e duas placas gráficas com 11 GB de memória cada, ajudando no processamento dos algoritmos de aprendizado.

Após a etapa de treino, foram realizados os testes para aferir os modelos no tocante às métricas de desempenho para o conjunto de testes. Nesta etapa, percebeu-se que alguns modelos tornaram-se degenerados e acabaram prevendo apenas uma das classes. Duas hipóteses podem justificar a ocorrência desse problema: o ReLU \emph{dying problem}, quando a função de ativação ReLU foi utilizada; ou a tendência a permanência em mínimos locais durante o treinamento do modelo. Todas as CNNs que manifestaram este comportamento no conjunto de testes tiveram seus resultados descartados, pois as métricas obtidas não refletiam o aprendizado do problema considerado.


\subsection{Resultados Obtidos com a CNN LeNet}
\label{sec:lenet}

A primeira fase do treinamento dos modelos foi conduzida utilizando a arquitetura LeNet. Nesta fase, foi realizada uma busca em \emph{grid} por todos os hiperparâmetros previamente definidos, conforme Seção \ref{sec:modelos}, gerando um total de $36$ modelos a serem treinados e testados. Para estes modelos, excluindo aqueles que se tornaram degenerados, utilizou-se a métrica \emph{F-score} como referência para um melhor desempenho. Em relação aos três otimizadores considerados, os modelos dispostos na Tabela \ref{tab:lenet} foram identificados como tendo melhor desempenho para a tarefa em questão.

\begin{table}[h!]
\centering
\caption{Detalhamento dos melhores modelos obtidos com a arquitetura LeNet, organizados de forma decrescente considerando o valor de acurácia.}
\label{tab:lenet}
\begin{tabular}{ccccc}
\toprule
\textbf{Otimizador} & \textbf{\emph{Patience}}  & \textbf{Função de Ativação} & \textbf{Acurácia} & \textbf{F-Score} \\
\midrule
RMSprop & 5 & \emph{Leaky} ReLU & $0.9865$ & $0.9755$ \\
SGD & 5 & ELU & $0.9787$ & $0.9619$ \\
Adam & 10 & ReLU & $0.9366$ & $0.8974$ \\
\bottomrule
\end{tabular}
\end{table}


Os gráficos da Figura \ref{fig:treinamento-lenet} denotam o histórico da perda (\emph{loss}) e acurácia para o conjunto de treinamento e validação destas redes. Nota-se que nenhuma delas chegou ao limite máximo de épocas possíveis, interrompendo o aprendizado por meio de \emph{early stopping}.


\begin{figure}[H]
	\centering
	\caption{Histórico de \emph{loss} e acurácia durante o treinamento dos melhores modelos obtidos com a arquitetura LeNet.}
	\subfloat[\emph{Loss} durante treinamento do melhor modelo com RMSprop.\label{subfig:lenet-rmsprop-loss}]{%
	\includegraphics[width=0.45\textwidth]{imgs/lenet-rmsprop-loss}
	}
	\subfloat[Acurácia durante treinamento do melhor modelo com RMSprop.\label{subfig:lenet-rmsprop-acc}]{%
	\includegraphics[width=0.45\textwidth]{imgs/lenet-rmsprop-acc}
	}
	\hfill
	\subfloat[\emph{Loss} durante treinamento do melhor modelo com SGD.\label{subfig:lenet-sgd-loss}]{%
	\includegraphics[width=0.45\textwidth]{imgs/lenet-sgd-loss}
	}
	\subfloat[Acurácia durante treinamento do melhor modelo com SGD.\label{subfig:lenet-sgd-acc}]{%
	\includegraphics[width=0.45\textwidth]{imgs/lenet-sgd-acc}
	}
	\hfill
	\subfloat[\emph{Loss} durante treinamento do melhor modelo com Adam.\label{subfig:lenet-adam-loss}]{%
	\includegraphics[width=0.45\textwidth]{imgs/lenet-adam-loss}
	}
	\subfloat[Acurácia durante treinamento do melhor modelo com Adam.\label{subfig:lenet-adam-acc}]{%
	\includegraphics[width=0.45\textwidth]{imgs/lenet-adam-acc}
	}
	\label{fig:treinamento-lenet}
\end{figure}

Examinando mais atentamente o desempenho destas redes no conjunto de testes, tem-se, então, as matrizes de confusão mostradas na Figura \ref{fig:matrizes-lenet}. Nestas matrizes, a soma das linhas representam a quantidade de assinaturas previstas para cada classe pelo modelo em questão, enquanto a soma das colunas denotam a quantidade de assinaturas existentes em cada classe.

\todo{Inverter xlabel e ylabel das matrizes}
\begin{figure}[H]
	\centering
	\caption{Matrizes de confusão dos melhores modelos obtidos com a arquitetura LeNet.}\label{fig:matrizes-lenet}
	\subfloat[Modelo com RMSprop\label{subfig:matriz-lenet-rmsprop}]{%
	\includegraphics[width=0.5\textwidth]{imgs/matriz-lenet-rmsprop}
	}
	\subfloat[Modelo com SGD\label{subfig:matriz-lenet-sgd}]{%
	\includegraphics[width=0.5\textwidth]{imgs/matriz-lenet-sgd}
	}
	\hfill
	\subfloat[Modelo com Adam\label{subfig:matriz-lenet-adam}]{%
	\includegraphics[width=0.5\textwidth]{imgs/matriz-lenet-adam}
	}
\end{figure}

%%%% ARGUMENTAÇÃO

Para esta arquitetura, é possível visualizar que os melhores modelos foram aqueles que possuíram o menor valor de \emph{patience}. Isso revela que houve uma oscilação no treinamento, de modo que o aprendizado de característica sobre o problema foi instável, resultando em uma parada precoce. Para contornar este efeito, pode ser necessário buscar ajustes de parâmetros (\emph{fine-tuning}) ou também avaliar outras arquiteturas nesta tarefa, nas quais este efeito é minimizado.

Percebeu-se que a maioria dos modelos tendeu a acertar mais os exemplos autênticos encontrados no conjunto de teste, no qual o melhor modelo com o otimizador Adam chegou a obter $100\%$ de aproveitamento para esta classe, conforme mostrado na matriz da Figura \ref{subfig:matriz-lenet-adam}. Isso pode ser explicado pela presença de assinaturas genuínas do mesmo indivíduo no conjunto de treinamento, porém, esta característica encontrada na partição dos dados, está de acordo com a tarefa definida para o escopo deste trabalho. Apesar disso, quando se observa também a inferência dessas CNNs para exemplos forjados, que são parte majoritária do conjunto de teste, o desempenho obtido com as redes LeNet foi significativamente bom para a tarefa considerada.

\subsection{Resultados Obtidos com a CNN AlexNet}
\label{sec:alexnet}
 %% Trabalhar aqui

 Para a AlexNet, assim como para a CNN anterior, foi realizada uma busca em \emph{grid} com os hiperparâmetros selecionados anteriormente, com vistas a obter os melhores modelos para cada abordagem de separação de dados, gerando assim, mais 36 modelos a serem avaliados quanto as suas métricas de desempenho.

 Considerando a métrica de \emph{F-score}, foram selecionados os melhores modelos e estes encontram-se listados na Tabela \ref{tab:alexnet}. Enquanto na Figura \ref{fig:treinamento-alexnet} pode-se observar os gráficos com os comportamentos dos valores de \emph{loss} e acurácia encontrado nos conjuntos de treinamento e validação durante o estágio de treino destes modelos.

 \begin{table}[h!]
 \centering
 \caption{Detalhamento dos melhores modelos obtidos com a arquitetura AlexNet, organizados de forma decrescente considerando o valor de Acurácia.}
 \label{tab:alexnet}
 \begin{tabular}{ccccc}
 \toprule
 \textbf{Otimizador} & \textbf{\emph{Patience}}  & \textbf{Função de Ativação} & \textbf{Acurácia} & \textbf{F-Score} \\
 \midrule
 Adam & 15 & ELU & $0.9654$ & $0.9393$ \\
 SGD & 10 & \emph{Leaky} ReLU & $0.9601$ & $0.9311$ \\
 RMSprop & 15 & \emph{Leaky} ReLU & $0.9397$ & $0.8975$ \\
 \bottomrule
 \end{tabular}
\end{table}


\begin{figure}[H]
	\centering
	\caption{Histórico de \emph{loss} e acurácia durante o treinamento dos melhores modelos obtidos com a arquitetura AlexNet.}
	\label{fig:treinamento-alexnet}
	\subfloat[\emph{Loss} durante treinamento do melhor modelo com Adam.\label{subfig:alexnet-adam-loss}]{%
	\includegraphics[width=0.45\textwidth]{imgs/alexnet-adam-loss}
	}
	\subfloat[Acurácia durante treinamento do melhor modelo com Adam.\label{subfig:alexnet-adam-acc}]{%
	\includegraphics[width=0.45\textwidth]{imgs/alexnet-adam-acc}
	}
	\hfill
	\subfloat[\emph{Loss} durante treinamento do melhor modelo com SGD.\label{subfig:alexnet-sgd-loss}]{%
	\includegraphics[width=0.45\textwidth]{imgs/alexnet-sgd-loss}
	}
	\subfloat[Acurácia durante treinamento do melhor modelo com SGD.\label{subfig:alexnet-sgd-acc}]{%
	\includegraphics[width=0.45\textwidth]{imgs/alexnet-sgd-acc}
	}
	\hfill
	\subfloat[\emph{Loss} durante treinamento do melhor modelo com RMSprop.\label{subfig:alexnet-rmsprop-loss}]{%
	\includegraphics[width=0.45\textwidth]{imgs/alexnet-rmsprop-loss}
	}
	\subfloat[Acurácia durante treinamento do melhor modelo com RMSprop.\label{subfig:alexnet-rmsprop-acc}]{%
	\includegraphics[width=0.45\textwidth]{imgs/alexnet-rmsprop-acc}
	}
\end{figure}

Mais uma vez, nota-se que houve uma oscilação nos treinamentos, de modo que o aprendizado de características sobre o problema foi instável, resultando em uma parada precoce, mesmo quando o valor de patience adotado é grande. Porém, desta vez, a tendência de previsão de falsos positivos aumentou quando comparada aos modelos obtidos pela LeNet, embora a quantidade de falsos negativos continue relativamente parecida. Estas previsões podem ser analisadas mais profundamente na Figura \ref{fig:matrizes-alexnet}, que demonstra as matrizes de confusão obtida em cada uma das CNNs apresentadas na Tabela \ref{tab:alexnet}.

\todo{Inverter xlabel e ylabel das matrizes}
\begin{figure}[H]
	\centering
	\caption{Matrizes de confusão dos melhores modelos obtidos com a arquitetura AlexNet.}
	\subfloat[Modelo com Adam\label{subfig:matriz-alexnet-adam}]{%
	\includegraphics[width=0.5\textwidth]{imgs/matriz-alexnet-adam}
	}
	\subfloat[Modelo com SGD\label{subfig:matriz-alexnet-sgd}]{%
	\includegraphics[width=0.5\textwidth]{imgs/matriz-alexnet-sgd}
	}
	\hfill
	\subfloat[Modelo com RMSprop\label{subfig:matriz-alexnet-rmsprop}]{%
	\includegraphics[width=0.5\textwidth]{imgs/matriz-alexnet-rmsprop}
	}
	\label{fig:matrizes-alexnet}
\end{figure}

De modo geral, apesar de possuir boas métricas, o melhor modelo encontrado pela arquitetura AlexNet, com um \emph{F-score} de $0.9393$, não foi suficiente para impugnar o melhor modelo obtido com a arquitetura LeNet ($0.9755$). Como demonstrado anteriormente, a quantidade camadas e parâmetros encontrados na AlexNet supera a quantidade encontrada na LeNet, desta forma os modelos obtidos por essa segunda arquitetura possuem uma eficiência melhor, tanto na questão de previsão de entradas quanto para o tempo de treinamento e tamanho de memória necessário para o seu armazenamento. Sendo assim, os resultados obtidos com a AlexNet foram insuficientes para a tarefa observada.
